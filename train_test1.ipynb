{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Loading file tang.pickle\n",
      "Number of training/test patches: (75739, 13365) 89104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30  train Loss: 1.2987 rec_loss: 0.3565 pred_loss: 0.9422: 100%|██████████| 3787/3787 [06:11<00:00, 10.19it/s]\n",
      "Epoch 1 / 30  val   Loss: 1.1906 rec_loss: 0.2526 pred_loss: 0.9379: 100%|██████████| 669/669 [00:30<00:00, 21.86it/s]\n",
      "Epoch 2 / 30  train Loss: 1.1585 rec_loss: 0.2245 pred_loss: 0.9339: 100%|██████████| 3787/3787 [05:41<00:00, 11.10it/s]\n",
      "Epoch 2 / 30  val   Loss: 1.1053 rec_loss: 0.1763 pred_loss: 0.9290: 100%|██████████| 669/669 [00:30<00:00, 21.75it/s]\n",
      "Epoch 3 / 30  train Loss: 1.0956 rec_loss: 0.1674 pred_loss: 0.9282: 100%|██████████| 3787/3787 [06:10<00:00, 10.22it/s]\n",
      "Epoch 3 / 30  val   Loss: 1.0784 rec_loss: 0.1540 pred_loss: 0.9243: 100%|██████████| 669/669 [00:30<00:00, 22.03it/s]\n",
      "Epoch 4 / 30  train Loss: 1.0748 rec_loss: 0.1522 pred_loss: 0.9225: 100%|██████████| 3787/3787 [05:40<00:00, 11.11it/s]\n",
      "Epoch 4 / 30  val   Loss: 1.0604 rec_loss: 0.1423 pred_loss: 0.9181: 100%|██████████| 669/669 [00:27<00:00, 24.18it/s]\n",
      "Epoch 5 / 30  train Loss: 1.0603 rec_loss: 0.1433 pred_loss: 0.9170: 100%|██████████| 3787/3787 [06:02<00:00, 10.44it/s]\n",
      "Epoch 5 / 30  val   Loss: 1.0474 rec_loss: 0.1342 pred_loss: 0.9132: 100%|██████████| 669/669 [00:28<00:00, 23.77it/s]\n",
      "Epoch 6 / 30  train Loss: 1.0459 rec_loss: 0.1335 pred_loss: 0.9125: 100%|██████████| 3787/3787 [06:02<00:00, 10.44it/s]\n",
      "Epoch 6 / 30  val   Loss: 1.0313 rec_loss: 0.1236 pred_loss: 0.9077: 100%|██████████| 669/669 [00:27<00:00, 23.96it/s]\n",
      "Epoch 7 / 30  train Loss: 1.0351 rec_loss: 0.1264 pred_loss: 0.9087: 100%|██████████| 3787/3787 [05:33<00:00, 11.35it/s]\n",
      "Epoch 7 / 30  val   Loss: 1.0244 rec_loss: 0.1188 pred_loss: 0.9056: 100%|██████████| 669/669 [00:27<00:00, 24.31it/s]\n",
      "Epoch 8 / 30  train Loss: 1.0262 rec_loss: 0.1201 pred_loss: 0.9060: 100%|██████████| 3787/3787 [06:02<00:00, 10.45it/s]\n",
      "Epoch 8 / 30  val   Loss: 1.0163 rec_loss: 0.1136 pred_loss: 0.9027: 100%|██████████| 669/669 [00:27<00:00, 24.17it/s]\n",
      "Epoch 9 / 30  train Loss: 1.0191 rec_loss: 0.1152 pred_loss: 0.9038: 100%|██████████| 3787/3787 [05:51<00:00, 10.77it/s]\n",
      "Epoch 9 / 30  val   Loss: 1.0089 rec_loss: 0.1078 pred_loss: 0.9011: 100%|██████████| 669/669 [00:20<00:00, 32.97it/s]\n",
      "Epoch 10 / 30  train Loss: 1.0133 rec_loss: 0.1112 pred_loss: 0.9022: 100%|██████████| 3787/3787 [06:00<00:00, 10.51it/s]\n",
      "Epoch 10 / 30  val   Loss: 1.0040 rec_loss: 0.1047 pred_loss: 0.8993: 100%|██████████| 669/669 [00:27<00:00, 24.28it/s]\n",
      "Epoch 11 / 30  train Loss: 1.0074 rec_loss: 0.1063 pred_loss: 0.9012: 100%|██████████| 3787/3787 [06:04<00:00, 10.38it/s]\n",
      "Epoch 11 / 30  val   Loss: 1.0002 rec_loss: 0.1021 pred_loss: 0.8981: 100%|██████████| 669/669 [00:29<00:00, 22.70it/s]\n",
      "Epoch 12 / 30  train Loss: 1.0026 rec_loss: 0.1022 pred_loss: 0.9003: 100%|██████████| 3787/3787 [05:46<00:00, 10.92it/s]\n",
      "Epoch 12 / 30  val   Loss: 0.9952 rec_loss: 0.0970 pred_loss: 0.8981: 100%|██████████| 669/669 [00:28<00:00, 23.31it/s]\n",
      "Epoch 13 / 30  train Loss: 0.9988 rec_loss: 0.0999 pred_loss: 0.8989: 100%|██████████| 3787/3787 [06:16<00:00, 10.07it/s]\n",
      "Epoch 13 / 30  val   Loss: 0.9962 rec_loss: 0.1003 pred_loss: 0.8958: 100%|██████████| 669/669 [00:28<00:00, 23.62it/s]\n",
      "Epoch 14 / 30  train Loss: 0.9956 rec_loss: 0.0976 pred_loss: 0.8980: 100%|██████████| 3787/3787 [04:55<00:00, 12.80it/s]\n",
      "Epoch 14 / 30  val   Loss: 0.9883 rec_loss: 0.0933 pred_loss: 0.8949: 100%|██████████| 669/669 [00:13<00:00, 51.19it/s]\n",
      "Epoch 15 / 30  train Loss: 0.9926 rec_loss: 0.0952 pred_loss: 0.8974: 100%|██████████| 3787/3787 [03:53<00:00, 16.19it/s]\n",
      "Epoch 15 / 30  val   Loss: 0.9858 rec_loss: 0.0912 pred_loss: 0.8946: 100%|██████████| 669/669 [00:13<00:00, 50.81it/s]\n",
      "Epoch 16 / 30  train Loss: 0.9906 rec_loss: 0.0938 pred_loss: 0.8968: 100%|██████████| 3787/3787 [03:55<00:00, 16.11it/s]\n",
      "Epoch 16 / 30  val   Loss: 0.9847 rec_loss: 0.0903 pred_loss: 0.8944: 100%|██████████| 669/669 [00:12<00:00, 52.32it/s]\n",
      "Epoch 17 / 30  train Loss: 0.9882 rec_loss: 0.0924 pred_loss: 0.8958: 100%|██████████| 3787/3787 [03:54<00:00, 16.14it/s]\n",
      "Epoch 17 / 30  val   Loss: 0.9795 rec_loss: 0.0868 pred_loss: 0.8927: 100%|██████████| 669/669 [00:12<00:00, 52.76it/s]\n",
      "Epoch 18 / 30  train Loss: 0.9860 rec_loss: 0.0908 pred_loss: 0.8952: 100%|██████████| 3787/3787 [03:53<00:00, 16.23it/s]\n",
      "Epoch 18 / 30  val   Loss: 0.9806 rec_loss: 0.0875 pred_loss: 0.8932: 100%|██████████| 669/669 [00:12<00:00, 51.61it/s]\n",
      "Epoch 19 / 30  train Loss: 0.9839 rec_loss: 0.0894 pred_loss: 0.8945: 100%|██████████| 3787/3787 [03:54<00:00, 16.17it/s]\n",
      "Epoch 19 / 30  val   Loss: 0.9768 rec_loss: 0.0854 pred_loss: 0.8913: 100%|██████████| 669/669 [00:13<00:00, 51.20it/s]\n",
      "Epoch 20 / 30  train Loss: 0.9826 rec_loss: 0.0884 pred_loss: 0.8942: 100%|██████████| 3787/3787 [03:53<00:00, 16.25it/s]\n",
      "Epoch 20 / 30  val   Loss: 0.9749 rec_loss: 0.0838 pred_loss: 0.8911: 100%|██████████| 669/669 [00:12<00:00, 51.57it/s]\n",
      "Epoch 21 / 30  train Loss: 0.9808 rec_loss: 0.0873 pred_loss: 0.8935: 100%|██████████| 3787/3787 [03:53<00:00, 16.24it/s]\n",
      "Epoch 21 / 30  val   Loss: 0.9785 rec_loss: 0.0868 pred_loss: 0.8917: 100%|██████████| 669/669 [00:12<00:00, 51.72it/s]\n",
      "Epoch 22 / 30  train Loss: 0.9807 rec_loss: 0.0873 pred_loss: 0.8934:   4%|▍         | 163/3787 [00:10<03:41, 16.33it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from data_loader import TANG\n",
    "from utils import *\n",
    "from model import AutoEncoderRNN, DecoderRNN\n",
    "from train_model import train_model\n",
    "\n",
    "device=torch.device('cuda:0')\n",
    "sequence_length = 100\n",
    "batch_size = 20\n",
    "\n",
    "dataset=TANG(seq_len=sequence_length, dataset_location='data/', normalize=False)\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.15 * dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "data_loaders={}\n",
    "dataset_sizes = {}\n",
    "data_loaders['train'] = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "data_loaders['val'] = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "dataset_sizes['val'] = len(test_indices)\n",
    "dataset_sizes['train'] = len(train_indices)\n",
    "print(\"Number of training/test patches:\", (len(train_indices),len(test_indices)), dataset.__len__())\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-4\n",
    "input_size = 69\n",
    "output_size = 16\n",
    "hidden_size = 32\n",
    "#model = nn.LSTM(input_size, output_size, num_layers = 3, bidirectional = False, batch_first = True)\n",
    "model = AutoEncoderRNN(input_size, hidden_size, output_size, num_layers = 3, seq_len = sequence_length, batch_size = batch_size, batch_first=True)\n",
    "#predictor = DecoderRNN(hidden_size, output_size, num_layers = 2, bidirectional =False, batch_first=True)\n",
    "model = model.to(device)\n",
    "#predictor = predictor.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#model, losses = train_model(model, predictor, device, data_loaders, dataset_sizes, criterion, optimizer1, optimizer2, num_epochs=num_epochs, batch_size = batch_size, train_predictor = False)\n",
    "model, losses = train_model(model, device, data_loaders, dataset_sizes, criterion, optimizer, num_epochs=num_epochs, batch_size = batch_size)\n",
    "with open('trained_models/model12_10:27PM.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('trained_models/loss12_10:27PM.pickle', 'wb') as f:\n",
    "    pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
